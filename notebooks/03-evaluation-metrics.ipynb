{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe81e33",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773756a7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bee3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Fixing routing issue\n",
    "project_root = os.path.abspath('..')\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5706b23",
   "metadata": {},
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc20bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>whats that egg website people talk about</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why ios</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we can assist you we recommend updating to io...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thats better than having an unstable connecti...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is probably one of the best airlines ive ever...</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text intent\n",
       "0           whats that egg website people talk about  Other\n",
       "1                                           why ios   Other\n",
       "2   we can assist you we recommend updating to io...  Other\n",
       "3   thats better than having an unstable connecti...  Other\n",
       "4   is probably one of the best airlines ive ever...  Other"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/golden_intent_labeled.csv\")\n",
    "\n",
    "# Dropping rows missing in cleaned_\n",
    "\n",
    "# Quick peep\n",
    "df[['cleaned_text', 'intent']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823e126",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fcd017",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "Y = df['intent']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "X_train = X_train.fillna(\"\")\n",
    "X_test = X_test.fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb59a5",
   "metadata": {},
   "source": [
    "## Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91990898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation with features ['TF-IDF'] ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Account Management       0.93      0.69      0.79        94\n",
      "           Billing       1.00      0.30      0.46        73\n",
      "         Complaint       1.00      0.18      0.31        33\n",
      "             Other       0.88      1.00      0.93      1369\n",
      "  Praise/Thank You       0.95      0.81      0.87       258\n",
      " Technical Support       1.00      0.67      0.80       155\n",
      "\n",
      "          accuracy                           0.90      1982\n",
      "         macro avg       0.96      0.61      0.70      1982\n",
      "      weighted avg       0.91      0.90      0.88      1982\n",
      "\n",
      "\n",
      "=== Evaluation with features ['TF-IDF', 'tweet_length'] ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Account Management       0.93      0.72      0.81        94\n",
      "           Billing       1.00      0.27      0.43        73\n",
      "         Complaint       1.00      0.18      0.31        33\n",
      "             Other       0.88      1.00      0.94      1369\n",
      "  Praise/Thank You       0.95      0.81      0.87       258\n",
      " Technical Support       0.98      0.66      0.79       155\n",
      "\n",
      "          accuracy                           0.90      1982\n",
      "         macro avg       0.96      0.61      0.69      1982\n",
      "      weighted avg       0.90      0.90      0.88      1982\n",
      "\n",
      "\n",
      "=== Evaluation with features ['TF-IDF', 'is_question'] ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Account Management       0.93      0.69      0.79        94\n",
      "           Billing       1.00      0.30      0.46        73\n",
      "         Complaint       1.00      0.18      0.31        33\n",
      "             Other       0.88      1.00      0.93      1369\n",
      "  Praise/Thank You       0.95      0.80      0.87       258\n",
      " Technical Support       1.00      0.67      0.80       155\n",
      "\n",
      "          accuracy                           0.89      1982\n",
      "         macro avg       0.96      0.61      0.70      1982\n",
      "      weighted avg       0.90      0.89      0.88      1982\n",
      "\n",
      "\n",
      "=== Evaluation with features ['TF-IDF', 'sentiment_score'] ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Account Management       0.93      0.68      0.79        94\n",
      "           Billing       1.00      0.30      0.46        73\n",
      "         Complaint       0.92      0.33      0.49        33\n",
      "             Other       0.88      1.00      0.93      1369\n",
      "  Praise/Thank You       0.96      0.79      0.87       258\n",
      " Technical Support       1.00      0.66      0.79       155\n",
      "\n",
      "          accuracy                           0.89      1982\n",
      "         macro avg       0.95      0.63      0.72      1982\n",
      "      weighted avg       0.90      0.89      0.88      1982\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4282      Praise/Thank You\n",
       " 7615                 Other\n",
       " 9703                 Other\n",
       " 2955      Praise/Thank You\n",
       " 7372      Praise/Thank You\n",
       "                ...        \n",
       " 3559     Technical Support\n",
       " 4822      Praise/Thank You\n",
       " 1732     Technical Support\n",
       " 4079    Account Management\n",
       " 4636                 Other\n",
       " Name: intent, Length: 1982, dtype: object,\n",
       " 4484     Technical Support\n",
       " 9102                 Other\n",
       " 8115    Account Management\n",
       " 3465                 Other\n",
       " 1353                 Other\n",
       "                ...        \n",
       " 1849                 Other\n",
       " 3366      Praise/Thank You\n",
       " 6089      Praise/Thank You\n",
       " 3940                 Other\n",
       " 9806                 Other\n",
       " Name: intent, Length: 7928, dtype: object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# X_train_length = X_train.apply(len).to_frame(name='sentiment_score')\n",
    "# X_test_length = X_test.apply(len).to_frame(name='sentiment_score')\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# train_length_scaled = scaler.fit_transform(X_train_length)\n",
    "# test_length_scaled = scaler.transform(X_test_length)\n",
    "\n",
    "\n",
    "# train_lengths_sparse = csr_matrix(train_length_scaled)\n",
    "# test_length_sparse = csr_matrix(test_length_scaled)\n",
    "\n",
    "\n",
    "# X_train_combined = hstack([X_train_tfidf, train_lengths_sparse])\n",
    "# X_test_combined = hstack([X_test_tfidf, test_length_sparse])\n",
    "\n",
    "from scripts.evaluate_model import evaluate_model\n",
    "\n",
    "# Baseline\n",
    "evaluate_model(df)\n",
    "\n",
    "# With tweet length\n",
    "evaluate_model(df, extra_feature_names=['tweet_length'])\n",
    "\n",
    "# With question flag\n",
    "evaluate_model(df, extra_feature_names=['is_question'])\n",
    "\n",
    "# With sentiment\n",
    "evaluate_model(df, extra_feature_names=['sentiment_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ff2f4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e59c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_comb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m baseline_model = LogisticRegression(max_iter=\u001b[32m1000\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m baseline_model.fit(\u001b[43mX_train_comb\u001b[49m, y_train)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_comb' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_model = LogisticRegression(max_iter=1000)\n",
    "baseline_model.fit(X_train_comb, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47871a83",
   "metadata": {},
   "source": [
    "# Predition and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a6072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Account Management       0.96      0.60      0.74        85\n",
      "           Billing       1.00      0.33      0.50        69\n",
      "         Complaint       1.00      0.14      0.24        37\n",
      "             Other       0.88      1.00      0.94      1350\n",
      "  Praise/Thank You       0.93      0.85      0.89       262\n",
      " Technical Support       0.95      0.69      0.80       173\n",
      "\n",
      "          accuracy                           0.90      1976\n",
      "         macro avg       0.95      0.60      0.68      1976\n",
      "      weighted avg       0.90      0.90      0.88      1976\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_comb)\n",
    "df[['cleaned_text', 'tweet_length']].head()\n",
    "print(classification_report(y_test, y_pred))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dd830e",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced6df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluate_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m evaluate_model(y_test, \u001b[43mmodel_predictions\u001b[49m, labels=label_order)\n",
      "\u001b[31mNameError\u001b[39m: name 'model_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from scripts.evaluate_model import evaluate_model\n",
    "\n",
    "evaluate_model(y_test, model_predictions, labels=label_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
